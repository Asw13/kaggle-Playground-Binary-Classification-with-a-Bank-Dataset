# ğŸ† Kaggle Tabular Playground Series â€“ Season 5, Episode 8  

**Competition:** [Playground Series â€” Season 5, Episode 8: Binary Classification with a Bank Dataset](https://www.kaggle.com/competitions/playground-series-s5e8)  

ğŸ¯ **Goal:** Predict whether a customer will subscribe to a marketing campaign (`y=1`) using a bank dataset.  
ğŸ“ˆ **Metric:** ROC AUC (achieved **0.96308** on leaderboard).  
âš ï¸ **Challenge:** Imbalanced dataset (~12% positive class).  

---

## ğŸ“‚ Repository Structure  

- ğŸ“’ `notebook.ipynb` â†’ Main pipeline (EDA, feature engineering, modeling, tuning, submission).  
- ğŸ“„ `submission.csv` â†’ Final predictions (`id`, `y`) for Kaggle.  
- ğŸ“ `README.md` â†’ Overview of project (this file).  

---

## ğŸ” Workflow Summary  

### 1ï¸âƒ£ Exploratory Data Analysis (EDA)  
- ğŸ“Š Used `describe()` and `info()` â†’ found **imbalance (~12% subscribed)** + outliers (e.g., balance, duration).  
- ğŸ“‰ Histograms + boxplots â†’ showed education, job, loan, and marital status patterns.  
- ğŸ”¥ Correlation heatmap â†’ `duration` had **0.52 correlation with y** â†’ key feature.  

### 2ï¸âƒ£ Feature Engineering & Preprocessing  
- ğŸ› ï¸ Built a **Preprocessing class** with:  
  - Duration transforms â†’ `duration_log`, `duration_sqrt`, `duration_per_campaign`.  
  - Cyclical encoding â†’ `month_sin`, `month_cos`, `day_sin`, `day_cos`.  
  - Binning â†’ age, balance, duration, and pdays into meaningful ranges.  
  - Binary flags â†’ `duration_favorable`, `pdays_favorable`.  
  - Combined features â†’ `poutcome_duration_pdays`.  
- âš–ï¸ Scaled continuous data with **RobustScaler** (handles outliers better).  
- ğŸ”‘ Encoding:  
  - LightGBM â†’ LabelEncoder  
  - Others â†’ mix of Label + OneHot depending on feature type.  

### 3ï¸âƒ£ Model Selection  
- ğŸ§ª Tested Logistic Regression, Random Forest, XGBoost, LightGBM.  
- âœ… **LightGBM** won â†’ handled imbalance + captured complex feature interactions.  
- âš–ï¸ Used stratified CV + `class_weight="balanced"` / `scale_pos_weight`.  

### 4ï¸âƒ£ Hyperparameter Tuning  
- ğŸ¤– Bayesian Optimization (BayesSearchCV) with 5-fold CV.  
- Best params:  
  - `learning_rate = 0.01078`  
  - `num_leaves = 379`  
  - `max_depth = 15`  

### 5ï¸âƒ£ Final Training & Submission  
- ğŸ” Trained with 5-fold cross-validation.  
- ğŸ›‘ Early stopping (500 rounds).  
- ğŸ“Š Averaged fold predictions â†’ stable + robust.  
- ğŸ… Public LB ROC AUC â†’ **0.96308**.  

---

## ğŸŒŸ Key Takeaways  

- ğŸš€ Feature engineering (esp. duration & pdays ranges) boosted performance.  
- âš–ï¸ Handling imbalance correctly was crucial.  
- ğŸ§  Bayesian tuning saved time vs grid search.  
- ğŸ”„ Averaging folds improved generalization.  

---

## ğŸ”® Future Improvements  

- ğŸ¤ Try model stacking/blending (LightGBM + XGBoost + CatBoost).  
- ğŸ“Œ Add SHAP for explainability.  
- ğŸ§‘â€ğŸ’» Explore deep tabular models (SAINT, FastAI).  

---

## ğŸ“š References  

- ğŸ”— [Competition Page](https://www.kaggle.com/competitions/playground-series-s5e8)  
- ğŸ“‘ My Kaggle Notebooks (https://www.kaggle.com/code/astik13aw/notebook16349fe563) 

---

âœ¨ *Thanks for reading! If you find this repo helpful, donâ€™t forget to â­ star it!* âœ¨ 

